# RAG Pipeline Technical Summary

## Core Technologies
- **Framework:** FastAPI
- **LLM:** Azure AI Foundry (e.g., GPT-4)
- **Embedding Model:** Ollama (`nomic-embed-text`)
- **Vector Store:** SQLite-Vec (local similarity search)
- **Document Parsing:** Docling (PDF/DOCX)

## Architecture
1.  **Ingestion (`POST /ingest`):**
    - `ingest_file` function is called.
    - **Docling:** Parses uploaded files (PDF/DOCX) into text chunks.
    - **Ollama:** Generates embeddings for each text chunk using `nomic-embed-text`.
    - **SQLite-Vec:** Stores the text chunks and their corresponding embeddings.
2.  **Chat (`POST /chat`):**
    - Query is received.
    - **Ollama:** Generates a vector for the query.
    - **SQLite-Vec:** Performs a similarity search to find relevant text chunks.
    - **Azure AI Foundry:** The retrieved text chunks are used as context for the LLM to generate an answer.

## Project Structure
- `main.py`: Main FastAPI application, defines `/ingest` and `/chat` endpoints.
- `rag.py`: Contains the core RAG logic (embedding, retrieval, generation).
- `ingestion.py`: Handles file parsing and chunking using Docling.
- `database.py`: Manages the SQLite-Vec database connection.
- `embeddings.py`: Interacts with the Ollama embedding model.
- `config.py`: Loads configuration from environment variables (Azure keys, Ollama URL, etc.).
- `test_rag.py`: Unit tests for the RAG pipeline.

## Setup & Execution
1.  `pip install -r requirements.txt`
2.  `python main.py` or `uvicorn main:app --reload`